{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analysis\n",
    "\n",
    "This notebook provides comprehensive evaluation and analysis of tracking results using MOT metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.evaluation import MOTEvaluator\n",
    "from src.evaluation.metrics import calculate_mota, calculate_idf1, calculate_track_quality_metrics\n",
    "from src.utils.visualization import plot_tracking_statistics\n",
    "from src.utils.file_utils import ensure_dir\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation paths\n",
    "gt_dir = \"../data/detections\"  # Ground truth detections\n",
    "results_dir = \"../output/tracking_results\"  # Tracking results\n",
    "plots_dir = \"../output/plots\"\n",
    "eval_output_dir = \"../output/evaluation\"\n",
    "\n",
    "# Create output directories\n",
    "ensure_dir(plots_dir)\n",
    "ensure_dir(eval_output_dir)\n",
    "\n",
    "print(f\"Ground truth directory: {gt_dir}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Plots output: {plots_dir}\")\n",
    "print(f\"Evaluation output: {eval_output_dir}\")\n",
    "\n",
    "# Check available result files\n",
    "if os.path.exists(results_dir):\n",
    "    result_files = list(Path(results_dir).glob(\"*.txt\"))\n",
    "    print(f\"\\nFound {len(result_files)} result files:\")\n",
    "    for f in result_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    print(f\"\\nResults directory not found: {results_dir}\")\n",
    "    print(\"Please run the tracking pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MOT evaluator\n",
    "evaluator = MOTEvaluator(\n",
    "    metrics=['HOTA', 'CLEAR', 'Identity'],\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"MOT Evaluator initialized with metrics:\")\n",
    "print(\"  - HOTA: Higher Order Tracking Accuracy\")\n",
    "print(\"  - CLEAR: Classical metrics (MOTA, MOTP)\")\n",
    "print(\"  - Identity: Identity-based metrics (IDF1)\")\n",
    "print(f\"  - IoU threshold: {evaluator.threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on available results\n",
    "evaluation_results = {}\n",
    "\n",
    "if os.path.exists(results_dir) and os.path.exists(gt_dir):\n",
    "    # Get list of sequences to evaluate\n",
    "    result_files = list(Path(results_dir).glob(\"*.txt\"))\n",
    "    \n",
    "    if result_files:\n",
    "        # Group results by tracker type\n",
    "        tracker_results = {}\n",
    "        \n",
    "        for result_file in result_files:\n",
    "            # Parse filename to extract sequence and tracker\n",
    "            filename = result_file.stem\n",
    "            \n",
    "            # Try to identify tracker type from filename\n",
    "            if 'yolo' in filename.lower():\n",
    "                tracker_type = 'YOLO'\n",
    "                sequence_name = filename.replace('_yolo', '')\n",
    "            elif 'botsort' in filename.lower():\n",
    "                tracker_type = 'BotSort'\n",
    "                sequence_name = filename.replace('_botsort', '')\n",
    "            else:\n",
    "                tracker_type = 'Unknown'\n",
    "                sequence_name = filename\n",
    "            \n",
    "            if tracker_type not in tracker_results:\n",
    "                tracker_results[tracker_type] = []\n",
    "            \n",
    "            tracker_results[tracker_type].append(sequence_name)\n",
    "        \n",
    "        print(f\"Found tracking results for {len(tracker_results)} tracker types:\")\n",
    "        for tracker, sequences in tracker_results.items():\n",
    "            print(f\"  - {tracker}: {len(sequences)} sequences\")\n",
    "        \n",
    "        # Evaluate each tracker\n",
    "        for tracker_type, sequences in tracker_results.items():\n",
    "            print(f\"\\n=== Evaluating {tracker_type} Tracker ===\")\n",
    "            \n",
    "            try:\n",
    "                # Create temporary results directory for this tracker\n",
    "                temp_results_dir = Path(eval_output_dir) / f\"temp_{tracker_type.lower()}\"\n",
    "                ensure_dir(str(temp_results_dir))\n",
    "                \n",
    "                # Copy results for this tracker\n",
    "                copied_sequences = []\n",
    "                for seq in sequences:\n",
    "                    src_file = Path(results_dir) / f\"{seq}_{tracker_type.lower()}.txt\"\n",
    "                    dst_file = temp_results_dir / f\"{seq}.txt\"\n",
    "                    \n",
    "                    if src_file.exists():\n",
    "                        import shutil\n",
    "                        shutil.copy2(src_file, dst_file)\n",
    "                        copied_sequences.append(seq)\n",
    "                \n",
    "                if copied_sequences:\n",
    "                    # Run evaluation\n",
    "                    metrics = evaluator.evaluate(\n",
    "                        gt_dir=gt_dir,\n",
    "                        results_dir=str(temp_results_dir),\n",
    "                        sequences=copied_sequences\n",
    "                    )\n",
    "                    \n",
    "                    evaluation_results[tracker_type] = metrics\n",
    "                    \n",
    "                    # Print results\n",
    "                    evaluator.print_results(metrics)\n",
    "                    \n",
    "                    # Save results\n",
    "                    results_file = Path(eval_output_dir) / f\"{tracker_type.lower()}_evaluation.json\"\n",
    "                    with open(results_file, 'w') as f:\n",
    "                        json.dump(metrics, f, indent=2)\n",
    "                    \n",
    "                    print(f\"Results saved to: {results_file}\")\n",
    "                \n",
    "                else:\n",
    "                    print(f\"No valid result files found for {tracker_type}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {tracker_type}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    else:\n",
    "        print(\"No result files found for evaluation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Required directories not found for evaluation.\")\n",
    "    print(\"Creating demo evaluation results...\")\n",
    "    \n",
    "    # Create demo results for visualization\n",
    "    evaluation_results = {\n",
    "        'YOLO': {\n",
    "            'MOTA': 65.2,\n",
    "            'MOTP': 78.1,\n",
    "            'IDF1': 70.5,\n",
    "            'precision': 85.3,\n",
    "            'recall': 76.8,\n",
    "            'false_positives': 234,\n",
    "            'false_negatives': 456,\n",
    "            'id_switches': 23\n",
    "        },\n",
    "        'BotSort': {\n",
    "            'MOTA': 68.7,\n",
    "            'MOTP': 79.3,\n",
    "            'IDF1': 73.2,\n",
    "            'precision': 87.1,\n",
    "            'recall': 78.9,\n",
    "            'false_positives': 198,\n",
    "            'false_negatives': 423,\n",
    "            'id_switches': 18\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"\\n=== Evaluation Complete ===\")\n",
    "print(f\"Evaluated {len(evaluation_results)} tracker(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison of evaluation metrics\n",
    "if evaluation_results:\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_metrics = pd.DataFrame(evaluation_results).T\n",
    "    \n",
    "    print(\"=== Metrics Comparison Table ===\")\n",
    "    display(df_metrics.round(2))\n",
    "    \n",
    "    # Save comparison table\n",
    "    df_metrics.to_csv(Path(eval_output_dir) / \"metrics_comparison.csv\")\n",
    "    print(f\"\\nComparison table saved to: {eval_output_dir}/metrics_comparison.csv\")\n",
    "    \n",
    "    # Create detailed comparison visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Primary metrics\n",
    "    primary_metrics = ['MOTA', 'MOTP', 'IDF1']\n",
    "    for i, metric in enumerate(primary_metrics):\n",
    "        if metric in df_metrics.columns:\n",
    "            ax = axes[0, i]\n",
    "            df_metrics[metric].plot(kind='bar', ax=ax, color=['skyblue', 'lightcoral'])\n",
    "            ax.set_title(f'{metric} Comparison')\n",
    "            ax.set_ylabel(f'{metric} (%)')\n",
    "            ax.set_xlabel('Tracker')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, v in enumerate(df_metrics[metric]):\n",
    "                ax.text(j, v + 1, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Secondary metrics\n",
    "    secondary_metrics = ['precision', 'recall', 'id_switches']\n",
    "    for i, metric in enumerate(secondary_metrics):\n",
    "        if metric in df_metrics.columns:\n",
    "            ax = axes[1, i]\n",
    "            if metric == 'id_switches':\n",
    "                # Lower is better for ID switches\n",
    "                colors = ['lightgreen' if v == df_metrics[metric].min() else 'lightcoral' for v in df_metrics[metric]]\n",
    "            else:\n",
    "                # Higher is better for precision/recall\n",
    "                colors = ['lightgreen' if v == df_metrics[metric].max() else 'lightcoral' for v in df_metrics[metric]]\n",
    "            \n",
    "            df_metrics[metric].plot(kind='bar', ax=ax, color=colors)\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison')\n",
    "            ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "            ax.set_xlabel('Tracker')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, v in enumerate(df_metrics[metric]):\n",
    "                if metric == 'id_switches':\n",
    "                    ax.text(j, v + 0.5, f'{int(v)}', ha='center', va='bottom')\n",
    "                else:\n",
    "                    ax.text(j, v + 1, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    comparison_plot_path = Path(plots_dir) / \"evaluation_comparison.png\"\n",
    "    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Comparison plot saved to: {comparison_plot_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No evaluation results available for comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of tracking performance\n",
    "if evaluation_results:\n",
    "    print(\"=== Detailed Performance Analysis ===\")\n",
    "    \n",
    "    for tracker_name, metrics in evaluation_results.items():\n",
    "        print(f\"\\n--- {tracker_name} Tracker Analysis ---\")\n",
    "        \n",
    "        # Overall performance assessment\n",
    "        if 'MOTA' in metrics:\n",
    "            mota = metrics['MOTA']\n",
    "            if mota >= 75:\n",
    "                performance = \"Excellent\"\n",
    "            elif mota >= 65:\n",
    "                performance = \"Good\"\n",
    "            elif mota >= 50:\n",
    "                performance = \"Fair\"\n",
    "            else:\n",
    "                performance = \"Poor\"\n",
    "            \n",
    "            print(f\"Overall Performance: {performance} (MOTA: {mota:.1f}%)\")\n",
    "        \n",
    "        # Detection quality\n",
    "        if 'precision' in metrics and 'recall' in metrics:\n",
    "            precision = metrics['precision']\n",
    "            recall = metrics['recall']\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "            \n",
    "            print(f\"Detection Quality:\")\n",
    "            print(f\"  - Precision: {precision:.1f}% (accuracy of detections)\")\n",
    "            print(f\"  - Recall: {recall:.1f}% (detection completeness)\")\n",
    "            print(f\"  - F1-Score: {f1_score:.1f}% (overall detection quality)\")\n",
    "        \n",
    "        # Identity consistency\n",
    "        if 'IDF1' in metrics:\n",
    "            idf1 = metrics['IDF1']\n",
    "            if idf1 >= 70:\n",
    "                id_quality = \"Excellent\"\n",
    "            elif idf1 >= 60:\n",
    "                id_quality = \"Good\"\n",
    "            elif idf1 >= 50:\n",
    "                id_quality = \"Fair\"\n",
    "            else:\n",
    "                id_quality = \"Poor\"\n",
    "            \n",
    "            print(f\"Identity Consistency: {id_quality} (IDF1: {idf1:.1f}%)\")\n",
    "        \n",
    "        # Error analysis\n",
    "        if 'false_positives' in metrics and 'false_negatives' in metrics:\n",
    "            fp = metrics['false_positives']\n",
    "            fn = metrics['false_negatives']\n",
    "            total_errors = fp + fn\n",
    "            \n",
    "            print(f\"Error Analysis:\")\n",
    "            print(f\"  - False Positives: {fp} ({fp/(fp+fn)*100:.1f}% of errors)\")\n",
    "            print(f\"  - False Negatives: {fn} ({fn/(fp+fn)*100:.1f}% of errors)\")\n",
    "            print(f\"  - Total Errors: {total_errors}\")\n",
    "        \n",
    "        # ID switches\n",
    "        if 'id_switches' in metrics:\n",
    "            id_sw = metrics['id_switches']\n",
    "            print(f\"Identity Switches: {id_sw} (lower is better)\")\n",
    "    \n",
    "    # Best tracker recommendation\n",
    "    if len(evaluation_results) > 1:\n",
    "        print(\"\\n=== Tracker Recommendation ===\")\n",
    "        \n",
    "        # Score each tracker based on multiple criteria\n",
    "        tracker_scores = {}\n",
    "        \n",
    "        for tracker_name, metrics in evaluation_results.items():\n",
    "            score = 0\n",
    "            criteria_count = 0\n",
    "            \n",
    "            # MOTA weight: 30%\n",
    "            if 'MOTA' in metrics:\n",
    "                score += metrics['MOTA'] * 0.3\n",
    "                criteria_count += 30\n",
    "            \n",
    "            # IDF1 weight: 25%\n",
    "            if 'IDF1' in metrics:\n",
    "                score += metrics['IDF1'] * 0.25\n",
    "                criteria_count += 25\n",
    "            \n",
    "            # Precision weight: 20%\n",
    "            if 'precision' in metrics:\n",
    "                score += metrics['precision'] * 0.2\n",
    "                criteria_count += 20\n",
    "            \n",
    "            # Recall weight: 20%\n",
    "            if 'recall' in metrics:\n",
    "                score += metrics['recall'] * 0.2\n",
    "                criteria_count += 20\n",
    "            \n",
    "            # ID switches penalty: 5% (lower is better)\n",
    "            if 'id_switches' in metrics:\n",
    "                max_id_switches = max(m.get('id_switches', 0) for m in evaluation_results.values())\n",
    "                if max_id_switches > 0:\n",
    "                    id_switch_score = (1 - metrics['id_switches'] / max_id_switches) * 100\n",
    "                    score += id_switch_score * 0.05\n",
    "                    criteria_count += 5\n",
    "            \n",
    "            tracker_scores[tracker_name] = score\n",
    "        \n",
    "        # Find best tracker\n",
    "        best_tracker = max(tracker_scores, key=tracker_scores.get)\n",
    "        \n",
    "        print(f\"Recommended Tracker: {best_tracker}\")\n",
    "        print(f\"Overall Score: {tracker_scores[best_tracker]:.1f}/100\")\n",
    "        \n",
    "        print(\"\\nAll Tracker Scores:\")\n",
    "        for tracker, score in sorted(tracker_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {tracker}: {score:.1f}/100\")\n",
    "\n",
    "else:\n",
    "    print(\"No evaluation results available for detailed analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Radar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for comprehensive comparison\n",
    "if evaluation_results and len(evaluation_results) >= 2:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    metrics_to_plot = ['MOTA', 'MOTP', 'IDF1', 'precision', 'recall']\n",
    "    trackers = list(evaluation_results.keys())\n",
    "    \n",
    "    # Check which metrics are available\n",
    "    available_metrics = []\n",
    "    for metric in metrics_to_plot:\n",
    "        if all(metric in evaluation_results[tracker] for tracker in trackers):\n",
    "            available_metrics.append(metric)\n",
    "    \n",
    "    if len(available_metrics) >= 3:\n",
    "        # Create radar chart\n",
    "        angles = np.linspace(0, 2 * np.pi, len(available_metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        \n",
    "        for i, tracker in enumerate(trackers):\n",
    "            values = []\n",
    "            for metric in available_metrics:\n",
    "                value = evaluation_results[tracker][metric]\n",
    "                # Normalize to 0-100 scale\n",
    "                if metric == 'id_switches':\n",
    "                    # For ID switches, invert and normalize (lower is better)\n",
    "                    max_id_switches = max(evaluation_results[t].get('id_switches', 0) for t in trackers)\n",
    "                    value = (1 - value / max_id_switches) * 100 if max_id_switches > 0 else 100\n",
    "                \n",
    "                values.append(value)\n",
    "            \n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax.plot(angles, values, 'o-', linewidth=2, label=tracker, color=colors[i % len(colors)])\n",
    "            ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])\n",
    "        \n",
    "        # Customize the chart\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(available_metrics)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks([20, 40, 60, 80, 100])\n",
    "        ax.set_yticklabels(['20%', '40%', '60%', '80%', '100%'])\n",
    "        ax.grid(True)\n",
    "        \n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "        plt.title('Tracker Performance Comparison\\n(Radar Chart)', size=16, pad=20)\n",
    "        \n",
    "        radar_plot_path = Path(plots_dir) / \"tracker_radar_comparison.png\"\n",
    "        plt.savefig(radar_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Radar chart saved to: {radar_plot_path}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Not enough metrics available for radar chart.\")\n",
    "\n",
    "else:\n",
    "    print(\"Need at least 2 trackers for radar chart comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive evaluation report\n",
    "if evaluation_results:\n",
    "    print(\"=== Exporting Evaluation Report ===\")\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report = {\n",
    "        'evaluation_summary': {\n",
    "            'trackers_evaluated': list(evaluation_results.keys()),\n",
    "            'evaluation_metrics': list(set().union(*(d.keys() for d in evaluation_results.values()))),\n",
    "            'iou_threshold': evaluator.threshold\n",
    "        },\n",
    "        'detailed_results': evaluation_results,\n",
    "        'analysis': {\n",
    "            'best_overall': None,\n",
    "            'best_detection': None,\n",
    "            'best_identity': None,\n",
    "            'recommendations': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Determine best performers\n",
    "    if len(evaluation_results) > 1:\n",
    "        # Best overall (highest MOTA)\n",
    "        if all('MOTA' in metrics for metrics in evaluation_results.values()):\n",
    "            best_mota = max(evaluation_results, key=lambda x: evaluation_results[x]['MOTA'])\n",
    "            report['analysis']['best_overall'] = best_mota\n",
    "        \n",
    "        # Best detection (highest precision)\n",
    "        if all('precision' in metrics for metrics in evaluation_results.values()):\n",
    "            best_detection = max(evaluation_results, key=lambda x: evaluation_results[x]['precision'])\n",
    "            report['analysis']['best_detection'] = best_detection\n",
    "        \n",
    "        # Best identity (highest IDF1)\n",
    "        if all('IDF1' in metrics for metrics in evaluation_results.values()):\n",
    "            best_identity = max(evaluation_results, key=lambda x: evaluation_results[x]['IDF1'])\n",
    "            report['analysis']['best_identity'] = best_identity\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        for tracker, metrics in evaluation_results.items():\n",
    "            tracker_rec = f\"Use {tracker} for: \"\n",
    "            use_cases = []\n",
    "            \n",
    "            if metrics.get('precision', 0) >= 85:\n",
    "                use_cases.append(\"high precision requirements\")\n",
    "            \n",
    "            if metrics.get('recall', 0) >= 80:\n",
    "                use_cases.append(\"comprehensive detection\")\n",
    "            \n",
    "            if metrics.get('IDF1', 0) >= 70:\n",
    "                use_cases.append(\"identity consistency\")\n",
    "            \n",
    "            if metrics.get('id_switches', float('inf')) <= 20:\n",
    "                use_cases.append(\"minimal ID switches\")\n",
    "            \n",
    "            if use_cases:\n",
    "                tracker_rec += \", \".join(use_cases)\n",
    "                recommendations.append(tracker_rec)\n",
    "        \n",
    "        report['analysis']['recommendations'] = recommendations\n",
    "    \n",
    "    # Save comprehensive report\n",
    "    report_file = Path(eval_output_dir) / \"comprehensive_evaluation_report.json\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    print(f\"Comprehensive report saved to: {report_file}\")\n",
    "    \n",
    "    # Create human-readable summary\n",
    "    summary_file = Path(eval_output_dir) / \"evaluation_summary.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"SOCCER TRACKING PIPELINE - EVALUATION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Evaluation Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Trackers Evaluated: {', '.join(evaluation_results.keys())}\\n\")\n",
    "        f.write(f\"IoU Threshold: {evaluator.threshold}\\n\\n\")\n",
    "        \n",
    "        # Results table\n",
    "        f.write(\"DETAILED RESULTS\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        \n",
    "        df_metrics = pd.DataFrame(evaluation_results).T\n",
    "        f.write(df_metrics.round(2).to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # Analysis\n",
    "        if report['analysis']['best_overall']:\n",
    "            f.write(f\"Best Overall Tracker: {report['analysis']['best_overall']}\\n\")\n",
    "        \n",
    "        if report['analysis']['recommendations']:\n",
    "            f.write(\"\\nRECOMMENDATIONS\\n\")\n",
    "            f.write(\"-\" * 15 + \"\\n\")\n",
    "            for rec in report['analysis']['recommendations']:\n",
    "                f.write(f\"• {rec}\\n\")\n",
    "    \n",
    "    print(f\"Human-readable summary saved to: {summary_file}\")\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n=== Final Evaluation Summary ===\")\n",
    "    if report['analysis']['best_overall']:\n",
    "        print(f\"🏆 Best Overall Tracker: {report['analysis']['best_overall']}\")\n",
    "    \n",
    "    if report['analysis']['recommendations']:\n",
    "        print(\"\\n📋 Key Recommendations:\")\n",
    "        for rec in report['analysis']['recommendations']:\n",
    "            print(f\"   • {rec}\")\n",
    "    \n",
    "    print(f\"\\n📁 All results saved to: {eval_output_dir}\")\n",
    "    print(f\"📊 Plots saved to: {plots_dir}\")\n",
    "\n",
    "else:\n",
    "    print(\"No evaluation results to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive evaluation and analysis:\n",
    "\n",
    "1. **MOT Metrics Evaluation** using industry-standard protocols\n",
    "2. **Detailed Performance Analysis** for each tracker\n",
    "3. **Visual Comparisons** with charts and plots\n",
    "4. **Tracker Recommendations** based on use cases\n",
    "5. **Comprehensive Reporting** with exportable results\n",
    "\n",
    "### Key Evaluation Metrics:\n",
    "- **MOTA**: Multiple Object Tracking Accuracy (overall performance)\n",
    "- **MOTP**: Multiple Object Tracking Precision (localization accuracy)\n",
    "- **IDF1**: Identity F1 Score (identity consistency)\n",
    "- **Precision/Recall**: Detection quality measures\n",
    "- **ID Switches**: Identity consistency measure\n",
    "\n",
    "### Files Generated:\n",
    "- **Metrics comparison table** (CSV format)\n",
    "- **Visualization plots** (PNG format)\n",
    "- **Comprehensive evaluation report** (JSON format)\n",
    "- **Human-readable summary** (TXT format)\n",
    "\n",
    "### Next Steps:\n",
    "- Use insights to optimize tracking parameters\n",
    "- Compare results across different datasets\n",
    "- Implement tracker selection based on use case\n",
    "- Monitor performance over time with regular evaluations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}